{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "# def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# print(findFiles('data/names/*.txt'))\n",
    "#@DatasetReader.register(\"datasetreader\")\n",
    "class PosDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for PoS tagging data, one sentence per line, like\n",
    "\n",
    "        The###DET dog###NN ate###V the###DET apple###NN\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "   \n",
    "    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n",
    "        tags = tags*len(list(tokens))\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"sentence\": sentence_field}\n",
    "\n",
    "        if tags:\n",
    "            label_field = LabelField(tags)\n",
    "            fields[\"labels\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "#      def text_to_instance(self, tokens: List[Token], id: str,\n",
    "#                          labels: np.ndarray) -> Instance:\n",
    "#         sentence_field = TextField(tokens, self.token_indexers)\n",
    "#         fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "#         id_field = MetadataField(id)\n",
    "#         fields[\"id\"] = id_field\n",
    "        \n",
    "#         label_field = ArrayField(array=labels)\n",
    "#         fields[\"label\"] = label_field\n",
    "\n",
    "#         return Instance(fields)\n",
    "    \n",
    "\n",
    "    def findFiles(self,path): return glob.glob(path)\n",
    "    \n",
    "    \n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "    \n",
    "    \n",
    "    def unicodeToAscii(self,s):\n",
    "     return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in self.all_letters\n",
    "     )\n",
    "\n",
    "\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "\n",
    "    # Read a file and split into lines\n",
    "    def readLines(self,filename):\n",
    "      lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "      return [self.unicodeToAscii(line) for line in lines]\n",
    "    \n",
    "    def _read(self, file_path: str)-> Iterator[Instance]:\n",
    "      for filename in self.findFiles(file_path):\n",
    "#             print(filename)\n",
    "\n",
    "            category = os.path.splitext(os.path.basename(filename))[0]\n",
    "#             print (category)\n",
    "            self.all_categories.append(category)\n",
    "            lines = self.readLines(filename)\n",
    "#             print (lines)\n",
    "            #for line in lines:\n",
    "            yield self.text_to_instance([Token(line) for line in lines], category)\n",
    "            self.category_lines[category] = lines\n",
    "#             print(self.category_lines[category][:5])\n",
    "#             print(filename)\n",
    "\n",
    "#      n_categories = len(self.all_categories)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "15it [00:00, 149.25it/s]\u001b[A\n",
      "18it [00:00, 169.54it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abe', 'Abukara', 'Adachi', 'Aida', 'Aihara']\n",
      "['Abbing', 'Abel', 'Abeln', 'Abt', 'Achilles']\n",
      "['Adam', 'Ahearn', 'Aodh', 'Aodha', 'Aonghuis']\n",
      "['Abl', 'Adsit', 'Ajdrna', 'Alt', 'Antonowitsch']\n",
      "['Khoury', 'Nahas', 'Daher', 'Gerges', 'Nazari']\n",
      "['Nguyen', 'Tron', 'Le', 'Pham', 'Huynh']\n",
      "['Abbas', 'Abbey', 'Abbott', 'Abdi', 'Abel']\n",
      "['Abreu', 'Albuquerque', 'Almeida', 'Alves', 'Araujo']\n",
      "['Ababko', 'Abaev', 'Abagyan', 'Abaidulin', 'Abaidullin']\n",
      "['Adamczak', 'Adamczyk', 'Andrysiak', 'Auttenberg', 'Bartosz']\n",
      "['Ang', 'AuYong', 'Bai', 'Ban', 'Bao']\n",
      "['Abana', 'Abano', 'Abarca', 'Abaroa', 'Abascal']\n",
      "['Aalsburg', 'Aalst', 'Aarle', 'Achteren', 'Achthoven']\n",
      "['Ahn', 'Baik', 'Bang', 'Byon', 'Cha']\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n",
      "['Adamidis', 'Adamou', 'Agelakos', 'Akrivopoulos', 'Alexandropoulos']\n",
      "['Abel', 'Abraham', 'Adam', 'Albert', 'Allard']\n",
      "['Smith', 'Brown', 'Wilson', 'Campbell', 'Stewart']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<allennlp.data.instance.Instance at 0x7fe2da3acbe0>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2da3d9e80>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2da369668>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2da38b2e8>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2da30b630>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2da30f748>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2da204550>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2da20a400>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9f804e0>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9f89630>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9f98e10>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9f2d320>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9f400b8>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9f43b38>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9ef1198>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9efafd0>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9f0d9b0>,\n",
       " <allennlp.data.instance.Instance at 0x7fe2d9f11f60>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = PosDatasetReader()\n",
    "reader.read('data/names/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
